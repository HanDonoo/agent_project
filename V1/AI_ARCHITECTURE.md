# AI Agent Architecture
## Company Employee Finder - Intelligent Routing System

---

## ğŸ¯ Overview

The system now includes **intelligent routing** that decides when to use AI (LLM) vs direct database queries, optimizing for both **speed** and **accuracy**.

---

## ğŸ—ï¸ Architecture Layers

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ROUTER (Query Classification)   â”‚
â”‚  - Analyzes query complexity        â”‚
â”‚  - Decides: Direct/Pattern/AI       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. STRATEGY EXECUTION              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Direct  â”‚ Pattern â”‚    AI     â”‚  â”‚
â”‚  â”‚ (Fast)  â”‚ (Fast)  â”‚ (Smart)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. TOOLS (Database Operations)     â”‚
â”‚  - find_by_email()                  â”‚
â”‚  - find_by_team()                   â”‚
â”‚  - find_by_skill()                  â”‚
â”‚  - find_by_responsibility()         â”‚
â”‚  - search_fulltext()                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. RESPONSE GENERATION             â”‚
â”‚  - Format results                   â”‚
â”‚  - Add context & next steps         â”‚
â”‚  - Include confidence level         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Response to User
```

---

## ğŸ”€ Router Decision Logic

### Query Type Classification

| Query Type | Example | Strategy | AI Needed? |
|------------|---------|----------|------------|
| **Direct Lookup** | "Find john.doe@sample.com" | Direct DB query | âŒ No |
| **Simple Search** | "Find someone in billing team" | Pattern matching | âŒ No |
| **Complex Intent** | "I need help with BIA provisioning" | AI understanding | âœ… Yes |
| **Conversational** | "Thanks!" or "Can you explain?" | AI response | âœ… Yes |
| **Ambiguous** | "Help" (too vague) | AI clarification | âœ… Yes |

### Decision Flow

```python
def route_query(query):
    # 1. Check for email pattern
    if contains_email(query):
        return DIRECT_LOOKUP  # No AI needed
    
    # 2. Check for conversational patterns
    if is_conversational(query):
        return CONVERSATIONAL  # AI needed
    
    # 3. Check for simple search patterns
    if matches_simple_pattern(query):
        return SIMPLE_SEARCH  # No AI needed
    
    # 4. Check if too short/ambiguous
    if word_count < 3:
        return AMBIGUOUS  # AI needed
    
    # 5. Default to complex intent
    return COMPLEX_INTENT  # AI needed
```

---

## ğŸ› ï¸ Tools System

Each tool is a discrete function that can be called independently:

### Tool 1: Direct Email Lookup
```python
find_by_email("john.doe@sample.com")
# Returns: Single employee or None
# Speed: ~10ms
```

### Tool 2: Team Search
```python
find_by_team("Network Infrastructure")
# Returns: List of employees in team
# Speed: ~50ms
```

### Tool 3: Role Search
```python
find_by_role("Network Engineer")
# Returns: List of employees with matching role
# Speed: ~50ms
```

### Tool 4: Skill Search
```python
find_by_skill("provisioning", min_confidence=0.6)
# Returns: List of employees with skill
# Speed: ~100ms
```

### Tool 5: Responsibility/Ownership Search
```python
find_by_responsibility("BIA provisioning")
# Returns: Primary owners, then backups
# Speed: ~100ms
```

### Tool 6: Full-Text Search
```python
search_fulltext("network provisioning Auckland")
# Returns: Ranked results using FTS5
# Speed: ~150ms
```

---

## ğŸ¤– LLM Integration

### When LLM is Used

1. **Complex Intent Understanding**
   - Query: "I need help setting up BIA provisioning for a new customer"
   - LLM extracts: domains=["provisioning", "BIA"], requirements={}, strategy="responsibility"

2. **Conversational Responses**
   - Query: "Thanks for the help!"
   - LLM generates: Friendly acknowledgment

3. **Clarification**
   - Query: "Help"
   - LLM asks: "What do you need help with? (e.g., finding someone, understanding a process)"

### LLM Providers Supported

#### Option 1: OpenAI (Cloud)
```bash
# .env configuration
ENABLE_LLM=True
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo
```

**Pros:**
- âœ… High quality understanding
- âœ… Fast response
- âœ… No local setup

**Cons:**
- âŒ Requires API key (costs money)
- âŒ Data sent to external service
- âŒ Requires internet

#### Option 2: Local LLM (Ollama, LocalAI)
```bash
# .env configuration
ENABLE_LLM=True
LLM_PROVIDER=local
LOCAL_LLM_ENDPOINT=http://localhost:11434/v1
LOCAL_LLM_MODEL=llama2
```

**Pros:**
- âœ… Free
- âœ… Privacy (data stays local)
- âœ… Works offline

**Cons:**
- âŒ Requires local setup
- âŒ Slower (depends on hardware)
- âŒ May need GPU for good performance

#### Option 3: No LLM (Pattern Matching Only)
```bash
# .env configuration
ENABLE_LLM=False
USE_AI_ROUTING=True  # Still uses router, but no LLM
```

**Pros:**
- âœ… Fast
- âœ… No external dependencies
- âœ… Predictable

**Cons:**
- âŒ Less flexible
- âŒ Can't handle complex queries as well

---

## ğŸ“Š Performance Comparison

| Strategy | Speed | Accuracy | Use Case |
|----------|-------|----------|----------|
| **Direct** | ğŸš€ 10ms | â­â­â­â­â­ 100% | Email lookup |
| **Pattern** | ğŸš€ 50-100ms | â­â­â­â­ 80% | Team/role search |
| **AI (Cloud)** | ğŸ¢ 500-1000ms | â­â­â­â­â­ 95% | Complex queries |
| **AI (Local)** | ğŸŒ 1000-3000ms | â­â­â­â­ 85% | Complex queries |

---

## ğŸ”„ Example Query Flows

### Example 1: Direct Lookup (No AI)
```
User: "Find john.doe@sample.com"
  â†“
Router: DIRECT_LOOKUP (confidence: 1.0)
  â†“
Tool: find_by_email("john.doe@sample.com")
  â†“
Response: [John Doe's info] (10ms)
```

### Example 2: Simple Search (No AI)
```
User: "Find someone in billing team"
  â†“
Router: SIMPLE_SEARCH (confidence: 0.8)
  â†“
Tool: find_by_team("billing")
  â†“
Response: [List of billing team members] (50ms)
```

### Example 3: Complex Intent (With AI)
```
User: "I need help with BIA provisioning"
  â†“
Router: COMPLEX_INTENT (confidence: 0.7)
  â†“
LLM: Extract intent â†’ {domains: ["provisioning", "BIA"], strategy: "responsibility"}
  â†“
Tool: find_by_responsibility("BIA provisioning")
  â†“
Response: [Primary owners, backups, escalation] (800ms)
```

---

## âš™ï¸ Configuration Options

### Recommended Configurations

#### 1. Production (High Quality)
```bash
USE_AI_ROUTING=True
ENABLE_LLM=True
LLM_PROVIDER=openai
OPENAI_MODEL=gpt-3.5-turbo
```
**Best for:** Production use with budget for API calls

#### 2. Production (Privacy-First)
```bash
USE_AI_ROUTING=True
ENABLE_LLM=True
LLM_PROVIDER=local
LOCAL_LLM_MODEL=llama2
```
**Best for:** Organizations with privacy requirements

#### 3. Development/Testing
```bash
USE_AI_ROUTING=True
ENABLE_LLM=False
```
**Best for:** Development, testing, or when LLM not needed

---

## ğŸ“ Key Design Decisions

### Why Router?
- **Efficiency**: Don't use expensive AI for simple queries
- **Speed**: Direct queries are 50-100x faster
- **Cost**: Save API costs by routing simple queries to DB
- **Reliability**: Pattern matching is more predictable for simple cases

### Why Support Multiple LLM Providers?
- **Flexibility**: Organizations have different requirements
- **Privacy**: Some need local-only processing
- **Cost**: Some want to avoid API costs
- **Availability**: Some don't have internet access

### Why Make LLM Optional?
- **Gradual Adoption**: Start simple, add AI later
- **Fallback**: System works even if LLM fails
- **Testing**: Easier to test without LLM dependency

---

## ğŸ“ˆ Future Enhancements

- [ ] **Function Calling**: Let LLM decide which tools to call
- [ ] **Multi-turn Conversations**: Remember context across queries
- [ ] **Learning**: Improve routing based on user feedback
- [ ] **Caching**: Cache LLM responses for common queries
- [ ] **Streaming**: Stream LLM responses for better UX

---

**Version**: 2.0.0  
**Last Updated**: 2026-01-18

